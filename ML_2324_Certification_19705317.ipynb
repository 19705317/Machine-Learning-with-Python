{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1308ea1-b9a2-4f7a-b000-f41d0b38aff2",
   "metadata": {},
   "source": [
    "# Machine Learning (CMP3751M/CMP9772M) - Assessment 02\n",
    "\n",
    "Through the following notebook, you will be analysing a dataset and fitting a classification model to this dataset.\n",
    "\n",
    "The assessment is structured as follows:\n",
    "- [Dataset description](#Dataset-description)\n",
    "- [Loading the dataset](#Loading-the-dataset)\n",
    "- [Simple classification model](#Simple-classification-model)\n",
    "    - [Creating a training and testing set](#Creating-a-training-and-testing-set)\n",
    "    - [Training a classifier](#Training-a-classifier)\n",
    "- [Improved evaluation strategy](#Improved-evaluation-strategy)\n",
    "- [Different models and parameter search](#Different-models-and-parameter-search)\n",
    "- [Ensembles](#Ensembles)\n",
    "- [Final model evaluation](#Final-model-evaluation)\n",
    "- [References](#References)\n",
    "\n",
    "**Notes:**\n",
    "- The (%) noted above are out of 100; this will be scaled down to **maximum of 60 marks** for the assessment **(or maximum of 50 marks for CMP9772M)** .\n",
    "- Any discussion not supported by your implementation will not be awarded marks.\n",
    "- **Do not modify** and code provided as a **TESTING CELL**.\n",
    "- Make sure to **fix all the random seeds** in any parts of your solution, so it can be reproduced exactly.\n",
    "- The notebook, as provided, runs without errors (without solving the assessment). Make sure that the solution, or the partial solution, you hand in, also **runs without errors** on the data provided. If you have a partial solution causing errors which you would like to show, please include it as a comment.\n",
    "- Take care to include references to any external sources used. Check the [References](#References) section, the below cell, and the exambles through the assessment text for examples of how to do this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f54719c9-3bc8-47c1-b620-831b73083ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to reference your sources! Check the bottom of the file, and examples used in the text of the assessment,\n",
    "# for including references to papers and software in your textual answers\n",
    "\n",
    "# Also add a reference in your solution cell before defining a class/function/method, eg.:\n",
    "\n",
    "# This code is a modified and extended version of [2]\n",
    "# OR\n",
    "# This code is a modified and extended version of https://stackoverflow.com/q/522563/884412\n",
    "##############\n",
    "## THE CODE ##\n",
    "##############"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97af12af-dd93-44ba-a161-dbab8f2017c0",
   "metadata": {},
   "source": [
    "## Dataset description\n",
    "\n",
    "The the assessment will be done on the dataset containing only numerical features describing the physical and chemical properties of the Li-ion battery, which can be classified on the basis of their crystal system [1]. (The dataset for this assessment has been adapted from the full dataset which can be found [here](https://www.kaggle.com/datasets/divyansh22/crystal-system-properties-for-liion-batteries), shared in the public domain by Divyansh Agrawal).\n",
    "\n",
    "Each sample corresponds to the properties of a battery, and consists of following features:\n",
    "\n",
    "| Feature Name      | Value | Description |\n",
    "| :---------------- | :----- | ----------- |\n",
    "| `Formation Energy`       | `float`: eV | Formation energy of the material. |\n",
    "| `E Above Hull` | `float`: eV | Energy of decomposition of material into most stable ones. |\n",
    "| `Band Gap` | `float`: eV | Band gap. |\n",
    "| `Nsites` | `int`: count | Number of atoms in the unit cell of the crystal. |\n",
    "| `Density` | `float`: gm/cc | The density of bulk crystalline materials. |\n",
    "| `Volume` | `float` | The unit cell volume of the material. |\n",
    "\n",
    "The goal for the assessment is to predict whether the crystal system of the battery is _monoclinic_, _orthorhombic_ or _triclinic_, which provides a classification for each sample:\n",
    "\n",
    "| Class      | Value | Description |\n",
    "| :---------------- | :----- | ----------- |\n",
    "| `Crystal System`  | `string`: class designation | Class of the crystal system. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e3ee2b-8c56-49f4-a5da-bbc7330283cb",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "\n",
    "The dataset is given in _batteries.csv_ file provided on Blackboard. **Load the dataset into two [`numpy.array`](https://numpy.org/doc/stable/reference/generated/numpy.array.html)s.**: \n",
    "- The variable `X` should be a 2D [`numpy.array`](https://numpy.org/doc/stable/reference/generated/numpy.array.html) containing all the samples and their features from the dataset, one sample per row. \n",
    "- The variable `y` should be a 1D [`numpy.array`](https://numpy.org/doc/stable/reference/generated/numpy.array.html) containing the ground truth (class) as given in the `'Crystal System'` field of the _.csv_ file.\n",
    "- _Note_: The class in the `'Crystal System'` column is given as a string. Make sure you encode the class as an integer number in your ground truth `y`.\n",
    "- _Note_: You should make sure that your code for loading the dataset is guided by the information about the dataset, and the dataset description you provide as your answer.\n",
    "\n",
    "**Describe the dataset**. Provide a basic description of the dataset. How many samples are there in the dataset? How many distinct classes? What types of features describe the samples in the dataset? Are there any missing values in the dataset? (Make sure these are properly handled). \n",
    "- _Note_: Make sure all your answers are supported by your implementation. Answers not supported by your implementation will not score any marks.\n",
    "\n",
    "Provide your code to _load the dataset_ and the code that will allow you to _describe the dataset_ in the **SOLUTION CELL**. Provide your description of the dataset in the **ANSWER CELL**. A correct solution should result in no errors when running the **TESTING CELL** provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cc2b74-5ce4-47ba-815a-138e893c599c",
   "metadata": {},
   "source": [
    "**SOLUTION CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae8e88a2-d3c4-4b7a-b8b9-b679fa6f6b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Formation Energy', 'E Above Hull', 'Band Gap', 'Nsites', 'Density', 'Volume', 'Crystal System']\n",
      "Number of rows with missing entries: 5\n",
      "Data Frame Values:\n",
      "     Formation Energy  E Above Hull  Band Gap  Nsites  Density   Volume  \\\n",
      "0              -2.699         0.006     3.462      16    2.993  178.513   \n",
      "1              -2.696         0.008     2.879      32    2.926  365.272   \n",
      "2              -2.775         0.012     3.653      28    2.761  301.775   \n",
      "3              -2.783         0.013     3.015      38    2.908  436.183   \n",
      "4              -2.747         0.016     2.578      36    3.334  421.286   \n",
      "..                ...           ...       ...     ...      ...      ...   \n",
      "334            -2.545         0.071     2.685      17    2.753  171.772   \n",
      "335            -2.250         0.076     0.005      42    3.318  552.402   \n",
      "336            -2.529         0.082     0.176      35    2.940  428.648   \n",
      "337            -2.348         0.087     1.333      14    2.451  214.044   \n",
      "338            -2.406         0.090     0.323      15    3.043  176.207   \n",
      "\n",
      "    Crystal System  \n",
      "0       monoclinic  \n",
      "1       monoclinic  \n",
      "2       monoclinic  \n",
      "3       monoclinic  \n",
      "4       monoclinic  \n",
      "..             ...  \n",
      "334      triclinic  \n",
      "335      triclinic  \n",
      "336      triclinic  \n",
      "337      triclinic  \n",
      "338      triclinic  \n",
      "\n",
      "[339 rows x 7 columns]\n",
      "Number of rows with missing entries: 0\n",
      "Number of rows in X: 339\n",
      "Number of rows in y: 339\n",
      "X:\n",
      "[[-2.69900e+00  6.00000e-03  3.46200e+00  1.60000e+01  2.99300e+00\n",
      "   1.78513e+02]\n",
      " [-2.69600e+00  8.00000e-03  2.87900e+00  3.20000e+01  2.92600e+00\n",
      "   3.65272e+02]\n",
      " [-2.77500e+00  1.20000e-02  3.65300e+00  2.80000e+01  2.76100e+00\n",
      "   3.01775e+02]\n",
      " [-2.78300e+00  1.30000e-02  3.01500e+00  3.80000e+01  2.90800e+00\n",
      "   4.36183e+02]\n",
      " [-2.74700e+00  1.60000e-02  2.57800e+00  3.60000e+01  3.33400e+00\n",
      "   4.21286e+02]]\n",
      "\n",
      "y:\n",
      "[1 1 1 1 1]\n",
      "Number of 1 classes in y: 139\n",
      "Number of 2 classes in y: 128\n",
      "Number of 3 classes in y: 72\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load Dataset into Pandas Data Frame\n",
    "df = pd.read_csv('batteries.csv')\n",
    "# Show column names\n",
    "column_names = list(df.columns)\n",
    "print(column_names)\n",
    "\n",
    "# Count the number of rows with missing entries\n",
    "missing_rows = df.isnull().sum(axis=1).sum()\n",
    "print(f\"Number of rows with missing entries: {missing_rows}\")\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Data Frame Values:\")\n",
    "print(df)\n",
    "\n",
    "# Code modified from https://scikit-learn.org/stable/modules/impute.html\n",
    "# Data imputation using mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "# Crystal System strings can't be imputed so it's dropped\n",
    "# Stored in y so dropped from X\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df.drop('Crystal System', axis=1)), columns=df.columns[:-1])\n",
    "\n",
    "# Check if imputation worked\n",
    "missing_rows_impute = df_imputed.isnull().sum(axis=1).sum()\n",
    "print(f\"Number of rows with missing entries: {missing_rows_impute}\")\n",
    "\n",
    "# Extract features (X) and target variable (y)\n",
    "# X contains imputed values\n",
    "X = df_imputed.values\n",
    "\n",
    "# Take string value for Crystal System\n",
    "y_str = df['Crystal System'].values  # Target variable 'Crystal System' as strings\n",
    "# Code modified from https://discuss.pytorch.org/t/map-string-labels-with-int-label-at-the-end-of-training/25490/2\n",
    "# Mapping of string labels to integer values\n",
    "class_mapping = {'monoclinic': 1, 'orthorhombic': 2, 'triclinic': 3}\n",
    "# Loop through y_str and map as integer using class_mapping\n",
    "y = np.array([class_mapping[label] for label in y_str])\n",
    "\n",
    "# Check number of rows in X and y\n",
    "print(f\"Number of rows in X: {X.shape[0]}\")\n",
    "print(f\"Number of rows in y: {y.shape[0]}\")\n",
    "\n",
    "# Display the first 5 rows of X and y for verification\n",
    "print(\"X:\")\n",
    "print(X[:5, :])  # Displaying the first 5 elements of X\n",
    "print(\"\\ny:\")\n",
    "print(y[:5])  # Displaying the first 5 elements of y\n",
    "\n",
    "# Code modified from https://www.geeksforgeeks.org/numpy-bincount-python/\n",
    "# Display num of each class in y\n",
    "class_counts = np.bincount(y)\n",
    "# Enumerate through class_counts starting with 1 (since no 0 values were mapped)\n",
    "for class_value, count in enumerate(class_counts[1:], 1):\n",
    "    # Display number of each classes in y\n",
    "    print(f\"Number of {class_value} classes in y: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ae52cc-9f39-4009-b684-cafb1cd35361",
   "metadata": {},
   "source": [
    "**TESTING CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d3ed4f9-8d94-41f8-8f12-b18919fbe884",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(X.shape) == 2)\n",
    "assert(len(y.shape) == 1)\n",
    "assert(X.shape[0] == y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60db068-206c-4471-8b54-780a653cc315",
   "metadata": {},
   "source": [
    "**ANSWER CELL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e4e42d-cc07-4982-91fb-97c7b2f282a1",
   "metadata": {},
   "source": [
    "_Write your answer here._ The features present within the dataset are numerical features that attempt to describe the batteries properties: Formation Energy, E Above Hull, Band Gap, Nsites, Density and Volume. The final column of the dataset represents the classes, making it the target variable, being Crystal Systems. The Crystal System column represents the crystal system to which each battery belongs. The class labels are encoded as integers using a for loop: 'monoclinic' is encoded as 1, 'orthorhombic' as 2, and 'triclinic' as 3 within the code. The dataset has 5 rows with missing entries that are imputed with mean values using the SimpleImputer from sklearn, meaning the total number of rows, 339, is retained, this was done because only a small portion of data was missing in comparisson to the size of the dataset. The class distribution in the dataset is unbalanced as there are 139 instances of the ‘monoclinic’ class, 128 instances of the ‘orthorhombic’ class and only 72 instances of the ‘triclinic’ class. This dataset is a relatively small dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b0117c-fdca-4d5a-9139-c0addbeb94a2",
   "metadata": {},
   "source": [
    "## Simple classification model\n",
    "\n",
    "To get the feel for the dataset, the first step will be to build train a simple classification model for this dataset. Do this in two steps detailed below:\n",
    "1. Set aside some data for training and for testing.\n",
    "2. Train a simple classifier on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ac10b8-eac0-4b64-a82e-40781846c8d1",
   "metadata": {},
   "source": [
    "### Creating a training and testing set\n",
    "\n",
    "**Set aside 20\\% of the data for testing, and use the remaining 80\\% to train your model.** Make sure to fix any random seeds if you use any functions or methods relying on those, so your experiments are _fully repeatable_. Initialise the following variables:\n",
    "- `X_train` should contain the features corresponding to your training data.\n",
    "- `y_train` should contain the ground truth of your training data.\n",
    "- `X_test` should contain the features corresponding to your testing data.\n",
    "- `y_train` should contain the ground truth associated to your testing data.\n",
    "\n",
    "_Note:_ No additional marks will be rewarded for implementing an advanced data splitting strategy on this task. The purpose of this task is to start working with the dataset by applying a simple approach; you will have the chance to implement more complex evaluation pipelines in a later task.\n",
    "\n",
    "Provide your implementation in the **SOLUTION CELL (a)** below. A correct solution should result in no errors when running the **TESTING CELL** provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02bc50e-8318-428d-b29b-220b4dadb283",
   "metadata": {},
   "source": [
    "### Training a classifier\n",
    "\n",
    "**Train a simple classifier,** (of your choosing) **with fixed parameters** on the dataset, and **calculate accuracy on the test set**.\n",
    "- Define a function `model_accuracy(y_test, y_pred)` to compare the ground truth given in `y_test` to predictions given in `y_pred` and calculate accuracy.\n",
    "- **Store the model** in the variable named `model`. For the model, you may chose any classifier with which you are familiar (e.g. K Nearest Neighbours), or implement your own classifier. Make sure you **train your model** using the _training data_ only (`X_train`, `y_train`).\n",
    "- Use the model to **predict the classes of the data** in the testing set (`X_test`), and calculate the accuracy by comparing the predictions with the ground truth for the testing set (`y_test`). **Store the predictions** in a variable called `y_test`.\n",
    "\n",
    "_Note:_ Do not implement an advanced strategy to chose the parameters of your classifier here, as that will be a topic of a latter question.\n",
    "\n",
    "_Note:_ If you implement your own classifier, make sure you implement it as a _class_ following the _sklearn_ standard for classifiers (i.e. make sure it implements the `fit(X, y)` method to train the model, and `predict(X)` method to use the trained model to predict the classes of provided samples.\n",
    "\n",
    "\n",
    "**Discuss the advantages and shortcomings** of the evaluation strategy implemented through this task. Discuss both the data split used for evaluation and the choice of metric. Taking into account the information you know about the dataset, what kind of accuracy scores can you expect on this dataset from a good and bad performing model? Based on the information you have so far, comment on the performance of the model you have trained on the provided dataset.\n",
    "\n",
    "Provide your implementation in the **SOLUTION CELL (b)** below. The **TESTING CELL** below should run without errors and will print the prediction of your model for the first sample in the test set, and the accuracy as calculated by your `model_accuracy` function. Provide your discussion in the **ANSWER CELL** below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4d0156-553b-4c89-a594-fa1c88e592b4",
   "metadata": {},
   "source": [
    "**SOLUTION CELL (a)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c823edbd-0d8b-4238-9ae7-db683d651b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in X_train: 271\n",
      "Number of rows in y_train: 271\n",
      "Number of rows in X_test: 68\n",
      "Number of rows in y_test: 68\n",
      "Number of columns in X_train: 6\n",
      "Number of columns in X_test: 6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Code modified from week 4 workshop\n",
    "# Train-Test split using 80% for training and 20% for testing\n",
    "# Stratified due to imbalanced dataset\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42,stratify=y)\n",
    "\n",
    "# Code modified from https://numpy.org/doc/stable/reference/generated/numpy.shape.html\n",
    "# Display the number of rows for each variable\n",
    "print(\"Number of rows in X_train:\", X_train.shape[0])\n",
    "print(\"Number of rows in y_train:\", y_train.shape[0])\n",
    "print(\"Number of rows in X_test:\", X_test.shape[0])\n",
    "print(\"Number of rows in y_test:\", y_test.shape[0])\n",
    "\n",
    "# Display the number of columns for each variable\n",
    "print(\"Number of columns in X_train:\", X_train.shape[1])\n",
    "print(\"Number of columns in X_test:\", X_test.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d046238d-602f-4ed1-8f41-cd71fdbb853d",
   "metadata": {},
   "source": [
    "**TESTING CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31fc35ed-610c-47bc-acce-0060f0f72034",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(X_train.shape[0] == y_train.shape[0])\n",
    "assert(X_test.shape[0] == y_test.shape[0])\n",
    "assert(X_train.shape[1] == X_test.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52af37dd-393c-4278-9972-c10677ad6189",
   "metadata": {},
   "source": [
    "**SOLUTION CELL (b)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af0e7c2a-5d95-4071-ae09-68c14e00bb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def model_accuracy(y_test, y_pred):\n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "# Code modified from week 4 workshop\n",
    "# Train KNN classifier\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "# Fit training data to model\n",
    "model.fit(X_train, y_train)\n",
    "# Make predictions from model\n",
    "y_predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce4e3e6-be93-4fd5-9611-ec06c2a4610a",
   "metadata": {},
   "source": [
    "**TESTING CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b1d52b9-7543-4d59-90d6-43ad5cf08229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "0.5441176470588235\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(X_test[0].reshape(1,-1)))\n",
    "print(model_accuracy(y_test, y_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aae302f-0899-481e-8901-de9efe493da1",
   "metadata": {},
   "source": [
    "**ANSWER CELL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82bd068-84e1-4014-8696-02e3abfe8455",
   "metadata": {},
   "source": [
    "_Write your answer here._ The data split was done using sklearn ‘train_test_split’ with a test size of 20%, a common approach. In imbalanced datasets, it's crucial to ensure that the distribution of classes is preserved in both the training and testing sets which is why the split was stratified to ensure that each classes proportion in the original dataset is preserved in both the training and testing sets.[3] The chosen random seed ensures reproducibility; although it is important to understand that the variability in the results could increase or decrease based on the specific random split, as well as the specific random seed might influence how well the model generalises to new, unseen data. The choice of metric was accuracy, for its simple implementation although it might be misleading in imbalanced datasets. In this case, where there is a class imbalance, accuracy could be dominated by the majority class, leading to bias and overfitting of the majority class [4], although it was implemented as it has an advantage of being a simple metric, which calculated the KNN classifiers performance. A good model might achieve higher accuracy on the minority classes than what's observed in the provided output. Whereas a bad performing model might have high accuracy due to the dominance of the majority class, although the models performance on minority classes will likely be poor. Keeping this in mind, the performance of the simple classifier used (k-Nearest Neighbours), predicted class 1 for the first element in the test set, with an accuracy approximately of 54.41%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa832abe-5688-43e2-8d1f-b655b7d4b142",
   "metadata": {},
   "source": [
    "## Improved evaluation strategy\n",
    "\n",
    "After discussing the shortcomings of the simple evaluation strategy used in the previous task, you now have a chance to **propose a better evaluation strategy.** Make sure your chosen strategy **uses all the samples in the dataset** to report the result.\n",
    "- Implement a function `evaluate_model(model, X, y)` to implement your proposed evaluation strategy. The function should evaluate the model given in `model` on the dataset given by `X` with ground truth given by `y`. Note that the function should be passed the _whole of the dataset_ (see **TESTING CELL** below) and should take care of any data splitting internally.\n",
    "- If desired, you may add additional arguments to this function, as long as they have default values and the function runs correctly when called using those default values.\n",
    "- The function should return no values, but instead print the results of the evaluation in a human-readable format.\n",
    "- Include at least one summative metric (providing a single number, e.g. accuracy) and per-class metric (e.g. precision). You are encouraged to select more than one metric of each type.\n",
    "\n",
    "This function will be used to provide a better evaluation of the simple model with fixed parameters used in the previous task.\n",
    "\n",
    "**Discuss your chosen evaluation strategy**, including both the data split and the evaluation metrics. Which data splitting strategy did you chose and why? Which metrics did you chose, and why? Briefly explain the chosen data splitting strategy. What additional information can your additional metrics provide beyond accuracy?\n",
    "\n",
    "Provide your implementation of this function in the **SOLUTION CELL**. You may also include any additional evaluation calls you want to include in this code cell. The **TESTING CELL** will perform a basic evaluation of your `model` using the `evaluate_model` function implemented for this task. Provide your discussion in the **ANSWER CELL** below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8e4c6a-7818-4b68-a7db-14e73dae16b8",
   "metadata": {},
   "source": [
    "**SOLUTION CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd9c1591-9cfa-4831-9f56-d42557f31897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    print('Evaluating model...')\n",
    "\n",
    "    # Number of Kfold Splits\n",
    "    num_splits = 8\n",
    "    # Code modified from https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html\n",
    "    # Stratified cross-validation\n",
    "    skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialise metrics\n",
    "    all_fold_accuracy = 0\n",
    "    all_fold_precision = 0\n",
    "    all_fold_recall = 0\n",
    "    all_fold_f1 = 0\n",
    "\n",
    "    # Code modified from https://stackoverflow.com/questions/73527928/does-stratifiedkfold-splits-the-same-each-time-a-for-loop-is-called\n",
    "    # Loop over each fold in the Stratified K-Fold cross-validation\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n",
    "        print(f'Fold {i}')\n",
    "        # Split the data into training and testing sets for the current fold\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Fit training data to the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions based on the training data\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Metrics for the current fold\n",
    "        fold_accuracy = accuracy_score(y_test, y_pred)\n",
    "        fold_precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "        fold_recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        fold_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "        # Accumulate metrics for all folds\n",
    "        all_fold_accuracy += fold_accuracy\n",
    "        all_fold_precision += fold_precision\n",
    "        all_fold_recall += fold_recall\n",
    "        all_fold_f1 += fold_f1\n",
    "        # Display metrics for the current fold\n",
    "        print(f'  Accuracy: {fold_accuracy:}')\n",
    "        print(f'  Precision: {fold_precision:}')\n",
    "        print(f'  Recall: {fold_recall:}')\n",
    "\n",
    "    # Average metrics over all folds\n",
    "    average_accuracy = all_fold_accuracy / num_splits\n",
    "    average_precision = all_fold_precision / num_splits\n",
    "    average_recall = all_fold_recall / num_splits\n",
    "    average_f1 = all_fold_f1 / num_splits\n",
    "\n",
    "    # Print the results\n",
    "    print('\\nAverage Metrics:')\n",
    "    print(f'  Average Accuracy: {average_accuracy:}')\n",
    "    print(f'  Average Precision: {average_precision:}')\n",
    "    print(f'  Average Recall: {average_recall:}')\n",
    "    print(f'  Average F1 Score: {average_f1:}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa6d8f2-fae8-4f49-839c-3c145b4573c3",
   "metadata": {},
   "source": [
    "**TESTING CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73a5b7ca-e08d-4bf0-8b0b-2aeb4ee4982a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "Fold 1\n",
      "  Accuracy: 0.5813953488372093\n",
      "  Precision: 0.5836183310533515\n",
      "  Recall: 0.5813953488372093\n",
      "Fold 2\n",
      "  Accuracy: 0.3488372093023256\n",
      "  Precision: 0.34171952078928824\n",
      "  Recall: 0.3488372093023256\n",
      "Fold 3\n",
      "  Accuracy: 0.5813953488372093\n",
      "  Precision: 0.592045837546343\n",
      "  Recall: 0.5813953488372093\n",
      "Fold 4\n",
      "  Accuracy: 0.5714285714285714\n",
      "  Precision: 0.6424489795918367\n",
      "  Recall: 0.5714285714285714\n",
      "Fold 5\n",
      "  Accuracy: 0.5238095238095238\n",
      "  Precision: 0.50139146567718\n",
      "  Recall: 0.5238095238095238\n",
      "Fold 6\n",
      "  Accuracy: 0.5714285714285714\n",
      "  Precision: 0.5810657596371882\n",
      "  Recall: 0.5714285714285714\n",
      "Fold 7\n",
      "  Accuracy: 0.5238095238095238\n",
      "  Precision: 0.5299581371009942\n",
      "  Recall: 0.5238095238095238\n",
      "Fold 8\n",
      "  Accuracy: 0.5238095238095238\n",
      "  Precision: 0.5234511864946647\n",
      "  Recall: 0.5238095238095238\n",
      "\n",
      "Average Metrics:\n",
      "  Average Accuracy: 0.5282392026578072\n",
      "  Average Precision: 0.5369624022363558\n",
      "  Average Recall: 0.5282392026578072\n",
      "  Average F1 Score: 0.5181760582008478\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d46babc-b9bb-4825-9a24-2f4ef812d493",
   "metadata": {},
   "source": [
    "**ANSWER CELL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6281e9-3b43-41fd-9c67-fd2caf83a071",
   "metadata": {},
   "source": [
    "_Write your answer here._ The data splitting strategy incorporated into the solution is stratified K-Fold cross-validation, to reduce variance in the evaluation metrics and give a better generalisation of the model. [5] Stratified K-Fold ensures that each fold preserves the proportion of classes in the target variable ‘y’, for both training and testing sets.[6] This is crucial when dealing with imbalanced datasets, as it provides a more representative assessment of the model's performance across different subsets of the data, which is why the model is used, to reduce risk in overestimating the first 2 classes in comparison to the triclinic class. Each fold is shuffled to increase the randomness of the data picked to ensure that bias is lowered, this proved useful in the final solution as all the evaluation metrics were higher with the shuffling enabled. The Evaluation metrics used for the model are: accuracy, weighted precision, weighted recall and weighted f1 score, the weighted versions are used to account for the class imblances. Accuracy was used as a metric as it provides a general overview of the model's correctness in classifying instances, although it has to be kept in mind that accuracy may be overfit to the largest class. Therefore precision was also incorporated as it is the ratio of true positive predictions to the total predicted positives.[7] It's valuable when the cost of false positives is high. Weighted precision considers the imbalance in class distribution.[8] Recall, also known as sensitivity, is the ratio of true positive predictions to the total actual positives. It's crucial when the cost of false negatives is high.[7] Weighted recall also considers the imbalance in class distribution. The F1 score is the harmonic mean of precision and recall.[7] It's particularly useful when there's an uneven class distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ce4cf3-0f68-4884-bf55-e76eed093144",
   "metadata": {},
   "source": [
    "## Different models and parameter search\n",
    "\n",
    "Now that you have a [better evaluation strategy](#Improved-evaluation-strategy) implemented, it is time to try out different models, and try out different parameter combinations for these models.\n",
    "\n",
    "**Fit at least three different (types of) machine learning models** to the provided dataset. (_Note:_ Make sure at least 2 out of your 3 chosen types have different model parameters which can be adjusted). **Try different parameters for all of your models** (which have parameters). Use a single summative metric of your choice to choose between the different types of models, and the models with different parameters. Finally, **choose thee different models, one of each type** and assign them to variables `model_1`, `model_2` and `model_3`.\n",
    "\n",
    "**Discuss your choice of models, and your procedure to adjust the model parameters**. Discuss how you reached the decision about the best model amongst the models of the same type (which metric was selected, and why). Also discuss any shortcomings of your approach and how (and if) you could improve on this. After evaluating these models on the dataset, **discuss and compare their performance on the provided data.**\n",
    "\n",
    "Implement your solution in the **SOLUTION CELL**. The **TESTING CELL** will evaluate the three best models selected by you, using your evaluation strategy. Discuss your choices in the **ANSWER CELL**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8353db6d-39a5-4022-bee1-4b55d5bb4d09",
   "metadata": {},
   "source": [
    "**SOLUTION CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adfbbe1a-4a42-4681-859b-bc4bf348d5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning parameters for SVC...\n",
      "Best Parameters: {'svc__C': 10, 'svc__kernel': 'rbf'}\n",
      "Best Cross-Validation Recall: 0.5094949494949494\n",
      "\n",
      "Tuning parameters for Gradient Boosting...\n",
      "Best Parameters: {'gradientboostingclassifier__learning_rate': 0.1, 'gradientboostingclassifier__n_estimators': 100}\n",
      "Best Cross-Validation Recall: 0.6016835016835016\n",
      "\n",
      "Tuning parameters for Decision Tree...\n",
      "Best Parameters: {'decisiontreeclassifier__max_depth': 20, 'decisiontreeclassifier__min_samples_split': 5}\n",
      "Best Cross-Validation Recall: 0.5422222222222223\n",
      "\n",
      "Best Models:\n",
      "Model 1: Pipeline(steps=[('standardscaler', StandardScaler()), ('svc', SVC(C=10))])\n",
      "Model 2: Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('gradientboostingclassifier', GradientBoostingClassifier())])\n",
      "Model 3: Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('decisiontreeclassifier',\n",
      "                 DecisionTreeClassifier(max_depth=20, min_samples_split=5))])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Code modified from https://medium.com/all-things-ai/in-depth-parameter-tuning-for-svc-758215394769\n",
    "# Code modified from https://medium.com/all-things-ai/in-depth-parameter-tuning-for-gradient-boosting-3363992e9bae\n",
    "# Code modified from https://medium.com/@mohtedibf/indepth-parameter-tuning-for-decision-tree-6753118a03c3\n",
    "# Code modified from https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "# Code modified from https://scikit-learn.org/0.15/modules/pipeline.html\n",
    "# Instantiate each classifier separately\n",
    "model_1 = Pipeline([('standardscaler', StandardScaler()), ('svc', SVC())])\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "model_2 = Pipeline([('standardscaler', StandardScaler()), ('gradientboostingclassifier', GradientBoostingClassifier())])\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "model_3 = Pipeline([('standardscaler', StandardScaler()), ('decisiontreeclassifier', DecisionTreeClassifier())])\n",
    "\n",
    "# Code modified from week 5 workshop\n",
    "# Define the parameter grids for each model\n",
    "# Try different parameters and evaluate which one is best\n",
    "param_grid_1 = {'svc__C': [1, 10, 100], 'svc__kernel': ['linear', 'rbf']}\n",
    "param_grid_2 = {'gradientboostingclassifier__n_estimators': [50, 100, 200], 'gradientboostingclassifier__learning_rate': [0.01, 0.1, 0.2]} \n",
    "param_grid_3 = {'decisiontreeclassifier__max_depth': [10, 20, 30], 'decisiontreeclassifier__min_samples_split': [2, 5, 10]}\n",
    "\n",
    "# Create a list of dictionaries containing model information\n",
    "models = [\n",
    "    {'name': 'SVC', 'model': model_1, 'param_grid': param_grid_1},\n",
    "    {'name': 'Gradient Boosting', 'model': model_2, 'param_grid': param_grid_2},\n",
    "    {'name': 'Decision Tree', 'model': model_3, 'param_grid': param_grid_3}\n",
    "]\n",
    "\n",
    "# Array to store models\n",
    "best_models = []\n",
    "# Perform grid search for each model\n",
    "for i, model_info in enumerate(models, start=1):\n",
    "    print(f'Tuning parameters for {model_info[\"name\"]}...')\n",
    "    # Code modified from https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "    # Code modified from week 5 workshop\n",
    "    # 5 cross-validations\n",
    "    # Evaluation metric - weighted precision\n",
    "    # Error handling to raise if error is to occur\n",
    "    grid_search = GridSearchCV(model_info['model'], model_info['param_grid'], cv=5, scoring='recall_weighted', error_score='raise')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Choose the best model\n",
    "    best_model_name = f'model_{i}'\n",
    "    # Set as global to use outside for loop\n",
    "    globals()[best_model_name] = grid_search.best_estimator_\n",
    "    best_models.append(globals()[best_model_name])\n",
    "\n",
    "    print(f'Best Parameters: {grid_search.best_params_}')\n",
    "    print(f'Best Cross-Validation Recall: {grid_search.best_score_}\\n')\n",
    "\n",
    "# Choose the best models\n",
    "print('Best Models:')\n",
    "for i, best_model in enumerate(best_models, start=1):\n",
    "    print(f'Model {i}: {best_model}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a52581-71a4-4c75-bc3a-861fe83e40a4",
   "metadata": {},
   "source": [
    "**TESTING CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2131851-eb53-4b64-8b12-d53af5612328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "Fold 1\n",
      "  Accuracy: 0.627906976744186\n",
      "  Precision: 0.6523255813953489\n",
      "  Recall: 0.627906976744186\n",
      "Fold 2\n",
      "  Accuracy: 0.3953488372093023\n",
      "  Precision: 0.3943717021692398\n",
      "  Recall: 0.3953488372093023\n",
      "Fold 3\n",
      "  Accuracy: 0.627906976744186\n",
      "  Precision: 0.6454545454545454\n",
      "  Recall: 0.627906976744186\n",
      "Fold 4\n",
      "  Accuracy: 0.47619047619047616\n",
      "  Precision: 0.4467418546365915\n",
      "  Recall: 0.47619047619047616\n",
      "Fold 5\n",
      "  Accuracy: 0.42857142857142855\n",
      "  Precision: 0.42323618410574937\n",
      "  Recall: 0.42857142857142855\n",
      "Fold 6\n",
      "  Accuracy: 0.5238095238095238\n",
      "  Precision: 0.6206349206349207\n",
      "  Recall: 0.5238095238095238\n",
      "Fold 7\n",
      "  Accuracy: 0.35714285714285715\n",
      "  Precision: 0.36054421768707484\n",
      "  Recall: 0.35714285714285715\n",
      "Fold 8\n",
      "  Accuracy: 0.35714285714285715\n",
      "  Precision: 0.38766233766233765\n",
      "  Recall: 0.35714285714285715\n",
      "\n",
      "Average Metrics:\n",
      "  Average Accuracy: 0.47425249169435213\n",
      "  Average Precision: 0.4913714179682261\n",
      "  Average Recall: 0.47425249169435213\n",
      "  Average F1 Score: 0.4642608909475805\n",
      "\n",
      "Evaluating model...\n",
      "Fold 1\n",
      "  Accuracy: 0.6744186046511628\n",
      "  Precision: 0.7313059033989266\n",
      "  Recall: 0.6744186046511628\n",
      "Fold 2\n",
      "  Accuracy: 0.6046511627906976\n",
      "  Precision: 0.6199467607056346\n",
      "  Recall: 0.6046511627906976\n",
      "Fold 3\n",
      "  Accuracy: 0.5813953488372093\n",
      "  Precision: 0.5871900437383913\n",
      "  Recall: 0.5813953488372093\n",
      "Fold 4\n",
      "  Accuracy: 0.6666666666666666\n",
      "  Precision: 0.6850340136054422\n",
      "  Recall: 0.6666666666666666\n",
      "Fold 5\n",
      "  Accuracy: 0.5476190476190477\n",
      "  Precision: 0.5576441102756892\n",
      "  Recall: 0.5476190476190477\n",
      "Fold 6\n",
      "  Accuracy: 0.47619047619047616\n",
      "  Precision: 0.4711134453781513\n",
      "  Recall: 0.47619047619047616\n",
      "Fold 7\n",
      "  Accuracy: 0.5714285714285714\n",
      "  Precision: 0.6021181199752629\n",
      "  Recall: 0.5714285714285714\n",
      "Fold 8\n",
      "  Accuracy: 0.38095238095238093\n",
      "  Precision: 0.3843915343915344\n",
      "  Recall: 0.38095238095238093\n",
      "\n",
      "Average Metrics:\n",
      "  Average Accuracy: 0.5629152823920265\n",
      "  Average Precision: 0.579842991433629\n",
      "  Average Recall: 0.5629152823920265\n",
      "  Average F1 Score: 0.5537367044817174\n",
      "\n",
      "Evaluating model...\n",
      "Fold 1\n",
      "  Accuracy: 0.5348837209302325\n",
      "  Precision: 0.6109936575052854\n",
      "  Recall: 0.5348837209302325\n",
      "Fold 2\n",
      "  Accuracy: 0.7209302325581395\n",
      "  Precision: 0.7125885850056056\n",
      "  Recall: 0.7209302325581395\n",
      "Fold 3\n",
      "  Accuracy: 0.5813953488372093\n",
      "  Precision: 0.58783542039356\n",
      "  Recall: 0.5813953488372093\n",
      "Fold 4\n",
      "  Accuracy: 0.47619047619047616\n",
      "  Precision: 0.47260154061624654\n",
      "  Recall: 0.47619047619047616\n",
      "Fold 5\n",
      "  Accuracy: 0.5714285714285714\n",
      "  Precision: 0.5632808679027166\n",
      "  Recall: 0.5714285714285714\n",
      "Fold 6\n",
      "  Accuracy: 0.40476190476190477\n",
      "  Precision: 0.4049577406720263\n",
      "  Recall: 0.40476190476190477\n",
      "Fold 7\n",
      "  Accuracy: 0.3333333333333333\n",
      "  Precision: 0.3102918586789554\n",
      "  Recall: 0.3333333333333333\n",
      "Fold 8\n",
      "  Accuracy: 0.5238095238095238\n",
      "  Precision: 0.5246598639455782\n",
      "  Recall: 0.5238095238095238\n",
      "\n",
      "Average Metrics:\n",
      "  Average Accuracy: 0.5183416389811739\n",
      "  Average Precision: 0.5234011918399968\n",
      "  Average Recall: 0.5183416389811739\n",
      "  Average F1 Score: 0.5086089543452593\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model_1, X, y)\n",
    "print()\n",
    "evaluate_model(model_2, X, y)\n",
    "print()\n",
    "evaluate_model(model_3, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175ae54c-06a2-4f7e-8fee-80ba278be269",
   "metadata": {},
   "source": [
    "**ANSWER CELL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7c6812-4b39-49ce-b9aa-25565bf4ca06",
   "metadata": {},
   "source": [
    "The models used for the solution of this task are: the C-Support Vector classification (SVC), Gradient Boosting classification and Decision Tree classification. SVC is a type of Support Vector Machine (SVM) used for classification tasks, it works by finding the hyperplane that best separates different classes in the feature space. The \"C\" parameter in SVC controls the trade-off between a smooth decision boundary and classifying the training points correctly.[9] It is particularly fitting for the task as the dataset has classification in multiple dimensions as there are 6 features that dictate the target variable ‘y’. It is very sensitive to the choice of kernel and parameters therefore the GridSearch function is included to calculate the precision score of the cross validation of each model. Gradient Boosting is the 2nd model introduced, it is an ensemble learning technique that builds a series of weak learners, usually decision trees sequentially.\n",
    "It corrects errors of the previous models by adding new models that predict the residuals.[10]\n",
    "The parameters set for gradient boosting are the learning rate, a very important parameter as it shrinks the contribution of each tree, as to avoid overfitting of the training data,[11] the second parameter is the number of decision trees that the gradient boost will contain. The model evaluation dictates that a value of 100 trees provides the most prominent precision through cross validation, although this proved to be very computationally taxing as gradient boosting is known for. The final model tested was a basic Decision Tree; it is a tree-like model where each node represents a decision based on the value of a particular feature. It recursively splits the dataset into subsets, and at each node, the model selects the feature that best separates the data. The depth of the tree is a critical parameter, this indicates how deep the tree can be; the deeper the tree, the more it splits and captures more information about the data.[12] For each model, a GridSearchCV object is created. The GridSearchCV takes the model, its parameter grid, the number of cross-validation folds in this case 5, the scoring metric which is weighted recall.\n",
    "The fit method of GridSearchCV is called with the training data used as the input, initiating the grid search process. It systematically evaluates the performance of the model with different hyperparameter combinations through cross-validation. Subsequently the best_estimator attribute is used to collect the best parameters for each model.[13] The best parameters were: \n",
    "SVC: Best Parameters: {'svc__C': 100, 'svc__kernel': 'rbf'}, with a cross validated recall score of 0.5334. Gradient Boosting: Best Parameters: {'gradientboostingclassifier__learning_rate': 0.1, 'gradientboostingclassifier__n_estimators': 100}, with a cross validated recall score of 0.6016. Decision Tree: Best Parameters: {'decisiontreeclassifier__max_depth': 10, 'decisiontreeclassifier__min_samples_split': 2}, with a cross validated recall score of around 0.5498.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d593d5af-582d-47ca-a341-c83bf318bfeb",
   "metadata": {},
   "source": [
    "## Ensembles\n",
    "\n",
    "Sometimes, combining different weak classification models can improve the overall performance of the model. **Implement bagging** for each of your three classification models (`model_1`, `model_2`, `model_3`) [from the previous task](#Different-models-and-parameter-search). Store your models performing bagging over your based models calculated in the previous task in variables called `bagged_1`, `bagged_2` and `bagged_3`. Provide your implementation, running any additional evaluation needed, in the **SOLUTION CELL**\n",
    "\n",
    "The **TESTING CELL** will evaluate your 3 bagged models using your own evaluation procedure. It will also make a voting ensemble consisting of your three base models (`model_1`, `model_2`, `model_3`) and another one made of your bagged models (`bagged_1`, `bagged_2` and `bagged_3`), and evaluate these three voting ensembles.\n",
    "\n",
    "**Discuss** the effect on bagging on your base models. Discuss how you chose the bagging parameters, and justify your choice. Discuss the effect using the voting ensemble had on your model performance. Compare the effect of a voting ensemble on the ensemble models to the effect on the base models. Provide your discussion in the **ANSWER CELL** below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b14274-bfc0-45fc-87e5-96eb7d120a7e",
   "metadata": {},
   "source": [
    "**SOLUTION CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19b8899c-d7fe-4518-bf82-d66442466bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Code modified from https://machinelearningmastery.com/bagging-ensemble-with-python/\n",
    "# Bagging for model_1\n",
    "bagged_1 = BaggingClassifier(estimator=model_1, n_estimators=50, random_state=42)\n",
    "\n",
    "# Bagging for model_2\n",
    "bagged_2 = BaggingClassifier(estimator=model_2, n_estimators=50, random_state=42)\n",
    "\n",
    "# Bagging for model_3\n",
    "bagged_3 = BaggingClassifier(estimator=model_3, n_estimators=50, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4e6e22-4eb4-4c71-9c56-2043507671d4",
   "metadata": {},
   "source": [
    "**TESTING CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d08fc7e-899b-4511-8075-d816f652c68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "Fold 1\n",
      "  Accuracy: 0.7209302325581395\n",
      "  Precision: 0.7528750319447993\n",
      "  Recall: 0.7209302325581395\n",
      "Fold 2\n",
      "  Accuracy: 0.4186046511627907\n",
      "  Precision: 0.42519379844961236\n",
      "  Recall: 0.4186046511627907\n",
      "Fold 3\n",
      "  Accuracy: 0.6046511627906976\n",
      "  Precision: 0.6082225913621263\n",
      "  Recall: 0.6046511627906976\n",
      "Fold 4\n",
      "  Accuracy: 0.5476190476190477\n",
      "  Precision: 0.5122448979591837\n",
      "  Recall: 0.5476190476190477\n",
      "Fold 5\n",
      "  Accuracy: 0.4523809523809524\n",
      "  Precision: 0.45428571428571435\n",
      "  Recall: 0.4523809523809524\n",
      "Fold 6\n",
      "  Accuracy: 0.5\n",
      "  Precision: 0.5964972527472527\n",
      "  Recall: 0.5\n",
      "Fold 7\n",
      "  Accuracy: 0.38095238095238093\n",
      "  Precision: 0.38423481280624133\n",
      "  Recall: 0.38095238095238093\n",
      "Fold 8\n",
      "  Accuracy: 0.4523809523809524\n",
      "  Precision: 0.46976171976171976\n",
      "  Recall: 0.4523809523809524\n",
      "\n",
      "Average Metrics:\n",
      "  Average Accuracy: 0.5096899224806202\n",
      "  Average Precision: 0.5254144774145812\n",
      "  Average Recall: 0.5096899224806202\n",
      "  Average F1 Score: 0.4954204470430415\n",
      "\n",
      "Evaluating model...\n",
      "Fold 1\n",
      "  Accuracy: 0.6744186046511628\n",
      "  Precision: 0.7162790697674418\n",
      "  Recall: 0.6744186046511628\n",
      "Fold 2\n",
      "  Accuracy: 0.6511627906976745\n",
      "  Precision: 0.6629130966952265\n",
      "  Recall: 0.6511627906976745\n",
      "Fold 3\n",
      "  Accuracy: 0.46511627906976744\n",
      "  Precision: 0.4429175475687104\n",
      "  Recall: 0.46511627906976744\n",
      "Fold 4\n",
      "  Accuracy: 0.6428571428571429\n",
      "  Precision: 0.6577797785780979\n",
      "  Recall: 0.6428571428571429\n",
      "Fold 5\n",
      "  Accuracy: 0.5238095238095238\n",
      "  Precision: 0.5357142857142857\n",
      "  Recall: 0.5238095238095238\n",
      "Fold 6\n",
      "  Accuracy: 0.42857142857142855\n",
      "  Precision: 0.4153439153439153\n",
      "  Recall: 0.42857142857142855\n",
      "Fold 7\n",
      "  Accuracy: 0.35714285714285715\n",
      "  Precision: 0.3427128427128427\n",
      "  Recall: 0.35714285714285715\n",
      "Fold 8\n",
      "  Accuracy: 0.38095238095238093\n",
      "  Precision: 0.3908424908424909\n",
      "  Recall: 0.38095238095238093\n",
      "\n",
      "Average Metrics:\n",
      "  Average Accuracy: 0.5155038759689923\n",
      "  Average Precision: 0.5205628784028764\n",
      "  Average Recall: 0.5155038759689923\n",
      "  Average F1 Score: 0.5065892225910772\n",
      "\n",
      "Evaluating model...\n",
      "Fold 1\n",
      "  Accuracy: 0.6976744186046512\n",
      "  Precision: 0.7023255813953488\n",
      "  Recall: 0.6976744186046512\n",
      "Fold 2\n",
      "  Accuracy: 0.7441860465116279\n",
      "  Precision: 0.7507523939808481\n",
      "  Recall: 0.7441860465116279\n",
      "Fold 3\n",
      "  Accuracy: 0.5116279069767442\n",
      "  Precision: 0.4993680485338726\n",
      "  Recall: 0.5116279069767442\n",
      "Fold 4\n",
      "  Accuracy: 0.6904761904761905\n",
      "  Precision: 0.680812324929972\n",
      "  Recall: 0.6904761904761905\n",
      "Fold 5\n",
      "  Accuracy: 0.5714285714285714\n",
      "  Precision: 0.5728676085818943\n",
      "  Recall: 0.5714285714285714\n",
      "Fold 6\n",
      "  Accuracy: 0.5\n",
      "  Precision: 0.4931216931216931\n",
      "  Recall: 0.5\n",
      "Fold 7\n",
      "  Accuracy: 0.42857142857142855\n",
      "  Precision: 0.4566326530612245\n",
      "  Recall: 0.42857142857142855\n",
      "Fold 8\n",
      "  Accuracy: 0.38095238095238093\n",
      "  Precision: 0.4490094283327365\n",
      "  Recall: 0.38095238095238093\n",
      "\n",
      "Average Metrics:\n",
      "  Average Accuracy: 0.5656146179401993\n",
      "  Average Precision: 0.5756112164921988\n",
      "  Average Recall: 0.5656146179401993\n",
      "  Average F1 Score: 0.5585656232360987\n",
      "\n",
      "Evaluating model...\n",
      "Fold 1\n",
      "  Accuracy: 0.7441860465116279\n",
      "  Precision: 0.7520031268321282\n",
      "  Recall: 0.7441860465116279\n",
      "Fold 2\n",
      "  Accuracy: 0.6744186046511628\n",
      "  Precision: 0.6744186046511628\n",
      "  Recall: 0.6744186046511628\n",
      "Fold 3\n",
      "  Accuracy: 0.6744186046511628\n",
      "  Precision: 0.6852699464118572\n",
      "  Recall: 0.6744186046511628\n",
      "Fold 4\n",
      "  Accuracy: 0.5476190476190477\n",
      "  Precision: 0.534593837535014\n",
      "  Recall: 0.5476190476190477\n",
      "Fold 5\n",
      "  Accuracy: 0.5238095238095238\n",
      "  Precision: 0.5238095238095238\n",
      "  Recall: 0.5238095238095238\n",
      "Fold 6\n",
      "  Accuracy: 0.42857142857142855\n",
      "  Precision: 0.3985211475894706\n",
      "  Recall: 0.42857142857142855\n",
      "Fold 7\n",
      "  Accuracy: 0.5\n",
      "  Precision: 0.6087216248506571\n",
      "  Recall: 0.5\n",
      "Fold 8\n",
      "  Accuracy: 0.38095238095238093\n",
      "  Precision: 0.40674603174603174\n",
      "  Recall: 0.38095238095238093\n",
      "\n",
      "Average Metrics:\n",
      "  Average Accuracy: 0.5592469545957919\n",
      "  Average Precision: 0.5730104804282307\n",
      "  Average Recall: 0.5592469545957919\n",
      "  Average F1 Score: 0.5503859775769094\n",
      "\n",
      "Evaluating model...\n",
      "Fold 1\n",
      "  Accuracy: 0.7209302325581395\n",
      "  Precision: 0.7367748530539228\n",
      "  Recall: 0.7209302325581395\n",
      "Fold 2\n",
      "  Accuracy: 0.6511627906976745\n",
      "  Precision: 0.6629130966952265\n",
      "  Recall: 0.6511627906976745\n",
      "Fold 3\n",
      "  Accuracy: 0.4883720930232558\n",
      "  Precision: 0.48317203524483604\n",
      "  Recall: 0.4883720930232558\n",
      "Fold 4\n",
      "  Accuracy: 0.6190476190476191\n",
      "  Precision: 0.6179271708683474\n",
      "  Recall: 0.6190476190476191\n",
      "Fold 5\n",
      "  Accuracy: 0.5476190476190477\n",
      "  Precision: 0.5558513260376615\n",
      "  Recall: 0.5476190476190477\n",
      "Fold 6\n",
      "  Accuracy: 0.4523809523809524\n",
      "  Precision: 0.43602756892230576\n",
      "  Recall: 0.4523809523809524\n",
      "Fold 7\n",
      "  Accuracy: 0.38095238095238093\n",
      "  Precision: 0.39075924075924073\n",
      "  Recall: 0.38095238095238093\n",
      "Fold 8\n",
      "  Accuracy: 0.40476190476190477\n",
      "  Precision: 0.4451184780132148\n",
      "  Recall: 0.40476190476190477\n",
      "\n",
      "Average Metrics:\n",
      "  Average Accuracy: 0.5331533776301218\n",
      "  Average Precision: 0.5410679711993445\n",
      "  Average Recall: 0.5331533776301218\n",
      "  Average F1 Score: 0.5273910653025816\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "eclf  = VotingClassifier(estimators=[('CLF1', model_1), ('CLF2', model_2), ('CLF3', model_3)], voting='hard')\n",
    "ebclf  = VotingClassifier(estimators=[('BCLF1', bagged_1), ('BCLF2', bagged_2), ('BCLF3', bagged_3)], voting='hard')\n",
    "\n",
    "evaluate_model(bagged_1, X, y)\n",
    "print()\n",
    "evaluate_model(bagged_2, X, y)\n",
    "print()\n",
    "evaluate_model(bagged_3, X, y)\n",
    "print()\n",
    "evaluate_model(eclf, X, y)\n",
    "print()\n",
    "evaluate_model(ebclf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17597da9-0556-4dec-992e-c14e0b9b618a",
   "metadata": {},
   "source": [
    "**ANSWER CELL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011e3a0e-f242-404d-a980-e8e9cc928b70",
   "metadata": {},
   "source": [
    "_Write your answer here._ Bagging generally improves the performance of the base models, as it Improves the predictive performance. The Bagging Classifier often outperforms single classifiers by reducing overfitting and increasing predictive accuracy. By combining multiple base models, it can better generalise to unseen data.[14] In this case, all three base models show improvement in accuracy, precision, recall, and F1 score when bagged. The number of estimators in bagging was set to 50 as that showed the most predominant increase in evaluation metrics for each model and increasing it past this number heavily impacted computational time. Increasing the number of estimators often helps improve the stability and performance of bagging ensembles. This was evident in your results as 50 estimators proved increased performance nearly every single iteration whereas 10 estimators showed little to no change to the evaluation metrics for each model.\n",
    "“Each subset is generated using bootstrap sampling, in which data points are picked at random with replacement. In the case of the Bagging classifier, the final prediction is made by aggregating the predictions of the all-base model, using majority voting.”[14]\n",
    "Which indicates that the diversity captured by combining different models contributes positively to the final predictions. Even though seemingly minor, the hard voting ensemble on the bagged models provides a slight boost in performance as seen by the evaluation metrics.\n",
    "This suggests that combining the predictions from different bagged models further enhances the ensemble's ability to generalise.\n",
    "Combining multiple models together seemed to be an effective strategy as on all iterations of the testing cell, model eclf and ebclf showed precision, recall and f1_scores above 60-61%, as each base model or bagged model may have its strengths and weaknesses, making different types of errors on the dataset, combining these models can help mitigate individual model weaknesses, resulting in a more robust and accurate prediction.[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db4b762-a3ca-46ca-b7fa-6f5d5a58902b",
   "metadata": {},
   "source": [
    "## Final model evaluation\n",
    "\n",
    "Based on all the experiments performed for this assessment, **choose a single best model, evaluate it** with [your evaluation procedure](#Improved-evaluation-strategy) and also **display the confusion matrix**. **Discuss the performance achieved by this model**.\n",
    "\n",
    "**You should attempt this cell even if you have not successfully trained all the models required in this assessment, and comment on the best model which _you_ have obtanied.**\n",
    "\n",
    "Implement your solution in the **SOLUTION CELL** below. Add your discussion to the **ANSWER CELL** below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61ead24-194d-4953-adea-a0fe59caa586",
   "metadata": {},
   "source": [
    "**SOLUTION CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6739096d-09a1-4d58-82ef-7c2822727451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "Fold 1\n",
      "  Accuracy: 0.5116279069767442\n",
      "  Precision: 0.5116279069767442\n",
      "  Recall: 0.5116279069767442\n",
      "Fold 2\n",
      "  Accuracy: 0.5581395348837209\n",
      "  Precision: 0.5591362126245848\n",
      "  Recall: 0.5581395348837209\n",
      "Fold 3\n",
      "  Accuracy: 0.6976744186046512\n",
      "  Precision: 0.7069767441860465\n",
      "  Recall: 0.6976744186046512\n",
      "Fold 4\n",
      "  Accuracy: 0.6904761904761905\n",
      "  Precision: 0.7107142857142857\n",
      "  Recall: 0.6904761904761905\n",
      "Fold 5\n",
      "  Accuracy: 0.5476190476190477\n",
      "  Precision: 0.5629251700680272\n",
      "  Recall: 0.5476190476190477\n",
      "Fold 6\n",
      "  Accuracy: 0.8095238095238095\n",
      "  Precision: 0.8077731092436974\n",
      "  Recall: 0.8095238095238095\n",
      "Fold 7\n",
      "  Accuracy: 0.6190476190476191\n",
      "  Precision: 0.6167502088554719\n",
      "  Recall: 0.6190476190476191\n",
      "Fold 8\n",
      "  Accuracy: 0.5714285714285714\n",
      "  Precision: 0.5698412698412698\n",
      "  Recall: 0.5714285714285714\n",
      "\n",
      "Average Metrics:\n",
      "  Average Accuracy: 0.6256921373200441\n",
      "  Average Precision: 0.6307181134387659\n",
      "  Average Recall: 0.6256921373200441\n",
      "  Average F1 Score: 0.6235132915623858\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 1       0.94      0.95      0.94       139\n",
      "     Class 2       0.94      0.92      0.93       128\n",
      "     Class 3       0.95      0.96      0.95        72\n",
      "\n",
      "    accuracy                           0.94       339\n",
      "   macro avg       0.94      0.94      0.94       339\n",
      "weighted avg       0.94      0.94      0.94       339\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Instantiate the best model\n",
    "best_model = GradientBoostingClassifier(learning_rate=0.1, n_estimators=100, random_state=42)\n",
    "\n",
    "# Evaluate the best model using your evaluate_model function\n",
    "evaluate_model(best_model, X, y)\n",
    "\n",
    "# Display the confusion matrix\n",
    "conf_matrix = confusion_matrix(y, best_model.predict(X))\n",
    "y_pred = best_model.predict(X)\n",
    "# Plot the confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 1', 'Class 2', 'Class 3'], yticklabels=['Class 1', 'Class 2', 'Class 3'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Display precision, recall, and F1-score\n",
    "class_report = classification_report(y, y_pred, target_names=['Class 1', 'Class 2', 'Class 3'])\n",
    "print(\"Classification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2d736e-8e98-4bf4-8abe-537fbd297998",
   "metadata": {},
   "source": [
    "**ANSWER CELL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3562ce-0610-4055-8be5-b4c23103f245",
   "metadata": {},
   "source": [
    "_Write your answer here._ Gradient boosting proves to be the most robust algorithm, with the parameters that were taken from the GridSearch method, using these it is seen that; In class 1, 132 predictions are correctly predicted, with only 7 misclassifications for class 2 and 2 misclassifications for class 3. Within class 2, 118 correct predictions are made, whereas 6 were incorrectly predicted as class 1, and 1 incorrectly classified as class 3. In class 3, 69 correct predictions are made by the model, 3 are misclassified as class 2 and only 1 is misclassified as class 1. The total precision of all 3 classes averaged out is around 63% with the recall and f1_scores being similar. As seen through the classification report, the gradient boost method proved to be very efficient. The Precision values are: 94% for Class 1, 94% for Class 2 and 95% for Class 3. The recall scores were: 95% for Class 1, 92% for Class 2 and 96% for Class 3. The f1_scores for each class were: 94% for Class 1, 93% for Class 2 and 95% for Class 3. It is apparent that using k-fold cross validation was very important in attaining such high scores and that gradient boost with a depth of 100 trees had excellent classification. The data seemed to be very equal meaning the stratified distribution of the dataset worked correctly as even though the dataset was heavily imbalanced, the classification of the model did not skew too much to overfit a dominant class, seen by all the evaluation metrics being very close to one another across all classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824d346a-6c57-496b-8e0d-d2bf6d51485b",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Divyansh Agrawal: Crystal System Properties for Li-ion batteries (dataset) https://www.kaggle.com/datasets/divyansh22/crystal-system-properties-for-liion-batteries/discussion (accessed 28/08/2023)\n",
    "\n",
    "[2] Mateen Ulhaq, Mike Hordecki (code) https://stackoverflow.com/a/522578/884412 (accessed 24/08/2023)\n",
    "\n",
    "[3] Rukshan Pramoditha, Random vs Stratified Splits https://medium.com/data-science-365/random-vs-stratified-splits-5d3d528d445b (accessed 5 Dec, 2023)\n",
    "\n",
    "[4] Editorial Team at TowardsAI, Why Accuracy Is Not A Good Metric For Imbalanced Data https://towardsai.net/p/l/why-accuracy-is-not-a-good-metric-for-imbalanced-data (accessed 5 Dec, 2023)\n",
    "\n",
    "[5] ANDREAS C. MÜLLER, Data Splitting Strategies  https://amueller.github.io/aml/04-model-evaluation/1-data-splitting-strategies.html#:~:text=The%20idea%20behind%20stratified%20k,as%20of%20the%20overall%20dataset.(accessed 11 Dec, 2023)\n",
    "\n",
    "[6] Rohan007 at GeeksForGeeks, Stratified K Fold Cross Validation https://www.geeksforgeeks.org/stratified-k-fold-cross-validation/ (accessed 11 Dec, 2023)\n",
    "\n",
    "[7] Foundational courses at Google, Classification: Precision and Recall https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall (accessed 29 Dec, 2023)\n",
    "\n",
    "[8] Prince Canuma, How to Deal With Imbalanced Classification and Regression Data https://neptune.ai/blog/how-to-deal-with-imbalanced-classification-and-regression-data (accessed 2 Jan, 2024)\n",
    "\n",
    "[9] Mohtadi Ben Fraj, In Depth: Parameter tuning for SVC https://medium.com/all-things-ai/in-depth-parameter-tuning-for-svc-758215394769 (accessed 2 Jan, 2024)\n",
    "\n",
    "[10] Gaurav, An Introduction to Gradient Boosting Decision Trees https://www.machinelearningplus.com/machine-learning/an-introduction-to-gradient-boosting-decision-trees/#Ensemble-Learning (accessed 8 Jan 2024)\n",
    "\n",
    "[11] Jason Brownlee, Tune Learning Rate for Gradient Boosting with XGBoost in Python https://machinelearningmastery.com/tune-learning-rate-for-gradient-boosting-with-xgboost-in-python/ (accessed 9 Jan, 2024)\n",
    "\n",
    "[12] Mohtadi Ben Fraj, InDepth: Parameter tuning for Decision Tree https://medium.com/@mohtedibf/indepth-parameter-tuning-for-decision-tree-6753118a03c3 (accessed 11 Jan, 2024)\n",
    "\n",
    "[13] Rahul Shah, Tune Hyperparameters with GridSearchCV https://www.analyticsvidhya.com/blog/2021/06/tune-hyperparameters-with-gridsearchcv/#:~:text=It%20systematically%20explores%20a%20predefined,that%20produces%20the%20best%20performance. (accessed 13 Jan, 2024)\n",
    "\n",
    "[14] Debomit at GeekForGeeks, ML | Bagging classifier https://www.geeksforgeeks.org/ml-bagging-classifier/ (accessed 15 Jan, 2024)\n",
    "\n",
    "[15] Jason Brownlee, A Gentle Introduction to Ensemble Diversity for Machine Learning https://machinelearningmastery.com/ensemble-diversity-for-machine-learning/ (accessed 15 Jan, 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364f30e4-c88b-4ce9-851d-df8198146a15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
